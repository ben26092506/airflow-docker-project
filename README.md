# ğŸš€ Airflow Docker Project  
A fully containerized Apache Airflow environment running with Docker Compose on WSL2 (Ubuntu).  
Includes a complete ETL pipeline, persistent storage, and a production-like Airflow architecture.


## ğŸ“Œ Overview

This project demonstrates how to build and run a professional Apache Airflow environment using Docker.  
It includes:

- Webserver, Scheduler, Worker & Triggerer  
- Redis + Postgres backend  
- Persistent volumes (DAGs, Logs, Data, Plugins, Config)  
- A complete ETL pipeline (extract â†’ transform â†’ load)  
- A clean, reproducible setup for portfolio or real-world deployment

This repository is ideal for:
- Learning Airflow internals  
- Demonstrating ETL / Orchestration skills  
- Building a foundation for bigger Data Engineering projects  


## ğŸ§± Architecture


              +------------------------+
              |      Webserver         |
              |  (UI & DAG rendering)  |
              +-----------+------------+
                          |
                          v
+-------------+    +------+-------+    +-------------+
|  Triggerer  | -> |  Scheduler   | -> |   Worker     |
+-------------+    +------+-------+    +-------------+
                          |
                          v
                   +--------------+
                   |   Postgres   |
                   |  Metadata DB |
                   +--------------+
                          |
                          v
                   +--------------+
                   |    Redis     |
                   |   Message    |
                   |     Queue    |
                   +--------------+

Volumes:
- dags/ â†’ /opt/airflow/dags
- logs/ â†’ /opt/airflow/logs
- data/ â†’ /opt/airflow/data
- plugins/ â†’ /opt/airflow/plugins
- config/ â†’ /opt/airflow/config

airflow-docker-project/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ hello_dag.py
â”‚   â””â”€â”€ etl_example.py
â”‚
â”œâ”€â”€ data/                  # CSV files generated by ETL
â”œâ”€â”€ logs/                  # Airflow logs (not in Git)
â”œâ”€â”€ plugins/               # Optional custom plugins
â”œâ”€â”€ config/                # Airflow configuration overrides
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .env                   # Local environment settings (NOT in Git)
â””â”€â”€ README.md

ğŸ› ï¸ Technology Stack

Apache Airflow 2.10.x
Docker & Docker Compose
Postgres 13 (Airflow metadata DB)
Redis (Celery message broker)
Celery Executor
WSL2 Ubuntu
Python 3.12 (inside containers)

âš™ï¸ Setup & Installation
1 Clone the repository
git clone https://github.com/<yourusername>/airflow-docker-project.git
cd airflow-docker-project

2 Create local .env file
AIRFLOW_UID=1000
AIRFLOW_PROJ_DIR=/home/ben/airflow-docker

3 Initialize Airflow
docker compose up airflow-init

4 Start all services
docker compose up -d

5 Access the Airflow UI
http://localhost:8080
User: airflow
Password: airflow

ğŸ”„ Run the ETL Pipeline

The project includes a complete ETL DAG:

Extract â†’ Transform â†’ Load

Key location inside containers:
DATA_DIR = "/opt/airflow/data"

Trigger it manually via UI:

Go to DAGs
Select etl_example
Click â–¶ Trigger DAG

Output CSVs will appear in:

data/raw.csv
data/processed.csv


![DAG Graph](images/dag_graph.png)
![Task Success](images/dag_task_successful.png)
